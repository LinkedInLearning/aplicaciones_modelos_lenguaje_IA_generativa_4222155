{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce14bd0b",
   "metadata": {},
   "source": [
    "## Instalar MCP"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3c587b3",
   "metadata": {},
   "source": [
    "uv add mcp-langchain-adapters \n",
    "uv add mcpmcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b536f67",
   "metadata": {},
   "source": [
    "## Crear MCP server simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5ecce",
   "metadata": {},
   "source": [
    "## Conectar LangGraph con el servidor MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c124f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"IT_REQUESTS_DB\"] = \"./it_requests_demo.json\"\n",
    "print(\"✅ IT_REQUESTS_DB =\", os.environ[\"IT_REQUESTS_DB\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc3351d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Modelo (cambia el provider/modelo a lo que uses en el curso)\n",
    "load_dotenv()\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    model_provider=\"anthropic\",\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "90c11584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['create_it_request', 'get_request_status']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Cliente que utiliza el MCP server\n",
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "async def get_mcp_tools():\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"it_actions\": {\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"./it_actions_mcp_server.py\"],\n",
    "                \"transport\": \"stdio\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    tools = await client.get_tools()\n",
    "    return tools\n",
    "\n",
    "tools = await get_mcp_tools()\n",
    "[tool.name for tool in tools]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40c663",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MessagesState\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Cambia el modelo si quieres (o usa variable de entorno)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Requiere OPENAI_API_KEY si usas openai:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai:gpt-4.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m SYSTEM = SystemMessage(content=\n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\"Eres un agente de Help Desk interno.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mTu trabajo NO es resolver todo con texto, sino ejecutar acciones cuando aplique usando tools.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_model\u001b[39m(state: MessagesState):\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Bind de tools MCP al modelo\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:451\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m     warnings.warn(\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    446\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    447\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    448\u001b[39m     )\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    457\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:475\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m model, model_provider = _parse_model(model, model_provider)\n\u001b[32m    474\u001b[39m creator_func = _get_chat_model_creator(model_provider)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreator_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:35\u001b[39m, in \u001b[36m_call\u001b[39m\u001b[34m(cls, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[BaseChatModel], **kwargs: Any) -> BaseChatModel:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# TODO: replace with operator.call when lower bounding to Python 3.11\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:117\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    116\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:996\u001b[39m, in \u001b[36mBaseChatOpenAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    986\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_async_client = httpx.AsyncClient(\n\u001b[32m    987\u001b[39m             proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy, verify=global_ssl_context\n\u001b[32m    988\u001b[39m         )\n\u001b[32m    989\u001b[39m     async_specific = {\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_async_client\n\u001b[32m    991\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_async_httpx_client(\n\u001b[32m   (...)\u001b[39m\u001b[32m    994\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m: async_api_key_value,\n\u001b[32m    995\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     \u001b[38;5;28mself\u001b[39m.root_async_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43masync_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28mself\u001b[39m.async_client = \u001b[38;5;28mself\u001b[39m.root_async_client.chat.completions\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/openai/_client.py:488\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    486\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    489\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m     )\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cambia el modelo si quieres (o usa variable de entorno)\n",
    "load_dotenv()\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    model_provider=\"anthropic\",\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "SYSTEM = SystemMessage(content=\n",
    "\"\"\"Eres un agente de Help Desk interno.\n",
    "Tu trabajo NO es resolver todo con texto, sino ejecutar acciones cuando aplique usando tools.\n",
    "\n",
    "Reglas:\n",
    "- Si el usuario pide instalar software => usa create_it_request con request_type=\"software_install\".\n",
    "- Si el usuario no puede entrar a una cuenta / pide reset => request_type=\"access_reset\".\n",
    "- Si el usuario pide reemplazo de dispositivo => request_type=\"device_replacement\".\n",
    "- Usa priority=\"high\" si el mensaje sugiere bloqueo crítico (no puede trabajar).\n",
    "- Usa dedupe_key cuando puedas (por ejemplo, un id de email o conversación).\n",
    "- Por defecto: dry_run=True salvo que el usuario confirme explícitamente que quiere crear la solicitud.\n",
    "\n",
    "Después de ejecutar la tool, resume el resultado al usuario e incluye el request_id.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # Bind de tools MCP al modelo\n",
    "    response = model.bind_tools(tools).invoke([SYSTEM] + state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_conditional_edges(\"call_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"call_model\")\n",
    "\n",
    "graph = builder.compile()\n",
    "print(\"✅ Graph compilado\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f38cf26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'coroutine' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m builder = StateGraph(MessagesState)\n\u001b[32m      3\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33mcall_model\u001b[39m\u001b[33m\"\u001b[39m, call_model)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m builder.add_node(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mToolNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      6\u001b[39m builder.add_edge(START, \u001b[33m\"\u001b[39m\u001b[33mcall_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m builder.add_conditional_edges(\u001b[33m\"\u001b[39m\u001b[33mcall_model\u001b[39m\u001b[33m\"\u001b[39m, tools_condition)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:772\u001b[39m, in \u001b[36mToolNode.__init__\u001b[39m\u001b[34m(self, tools, name, tags, handle_tool_errors, messages_key, wrap_tool_call, awrap_tool_call)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28mself\u001b[39m._wrap_tool_call = wrap_tool_call\n\u001b[32m    771\u001b[39m \u001b[38;5;28mself\u001b[39m._awrap_tool_call = awrap_tool_call\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaseTool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype[BaseTool]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'coroutine' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 4) Construir grafo mínimo: modelo -> tools (si aplica) -> modelo\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_conditional_edges(\"call_model\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"call_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c8621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAQAElEQVR4nOydB3wUxdvHZ/dqem+kkISQQEKJVEGkN6Ur/pEmRQThFVBAUQGpIggoohRRugLSuyJKEzBIEQwttJCEkEJCSHJpV3bfZ2+Ty4XcBS5h93bv5ks+x97M3N7d7u9m5nlm5hkpTdMIg7E2UoTBCAAsRIwgwELECAIsRIwgwELECAIsRIwgwEJ8ksxk9dWzuY/S1Vo1rVHrdGpESBGtRTSiCYKA/wgZTWuI8qdSmtYS8EKJnNapmQNCQtO6sgOKKYNI5oDQn5/NJUhEU0w6othkJh1OySQi5n/4Z/hIhhOySGS0TkMYf2aZAyGTkUpnSa1wh6ad3JEIIbAfkeV+QsnJ3Q8fPSymKVoml8iVpMKRBO1oSyhSiigt0uuJeZTKSa2aQqyuaJqQErSWuYZSJaktZnRkSJHICB0cwKGEQLrS68zmlgrRKJ2UMiqEd2fKgHqNbgv7SzBAyglKXeGuyZQkRSFNEVVSRGm0lEwhCQp37DHKD4kHLESUkaQ+8OOD4kKth6+yYWuXhi+7IVFDoWPbsxKvqYrytf6hDq9PCERiwN6FuH1pakZyUe36Lr3eEVP98SxkPdAcXJNapNK16+9Xv7kzEjZ2LcTvP7mrVEiGzaqNbJdrcQUnd6UHRTn1fNsfCRj7FeKaGYm1wh1fGWFrFaFJ1kxPbNbVs3Fb4fY67FSIq6beqRvr2mmgD7IbfpyW6Bvk0HusQOtFEtkfa2feC63nZFcqBEZ9HpZxv/DU7iwkSOxOiPu+TydJ1H2EoDtMHPHO3PDLpx4jQWJnQqRQyk3V8JmhyD4hUe16zhvmJiHhYV9C3LQgxTfEEdkxPd/xVz3W3LygQgLDvoSYm1XyxvhayL4Jqut05qDgeop2JMT9q9McnaQ8f+OPP/547969yHK6dOmSmpqKOKDH2wGqx1okMOxIiOlJJSHRTohfrl27hiwnLS0tJycHcYNUhhRK8s+tD5GQsCMhqot1zTp6Im44ffr0mDFj2rRp07dv35kzZ2ZlMW1fs2bNHjx4MHfu3Pbt28NTlUq1atWqYcOGscW+/vrr4uJi9uWdOnXasmXLO++8Ay85ceJEr169ILFPnz6TJ09GHODup0xLLEZCwl6EeOe/QvDauPtJEAfcuHFj4sSJzZs337Fjx0cffXTz5s1Zs2YhvTrhccaMGcePH4eDrVu3rl+/fujQoUuXLoXyR44cWb16NXsGmUy2e/fuqKio5cuXv/TSS1AAEqFNX7JkCeIA3xBFsUqHhIS9zEdMTyySyAjEDZcuXVIqlSNHjiRJ0t/fPzo6+vbt25WLDRkyBGq+sLAw9unly5fPnDkzYcIExMz7Itzc3KZMmYJ4ISBYeePvXCQk7EWIhSodKeFKiLGxsdDIvv/++y1btmzbtm1wcDC0sJWLQbX3999/Q8MNVaZWy5gLnp7lXQWQL+ILDx+ZTkchIWEvTTOtB3FDvXr1li1b5uPj8+233/br12/cuHFQ21UuBrnQFkOBPXv2nD9/fsSIEca5crkc8YZUUjqxVzDYixAdHKU0xeGlb926NfQF9+/fD73D3NxcqB3ZOs8A/Ax27tw5YMAAECI035CSn5+PrETuwxICC9Eq+AYptBquGqMLFy5Abw8OoFLs2bMnmLogMnDBGJfRaDRFRUW+vr7sU7VaffLkSWQlMpKLSSkWojWIauFM6+iSAk5aZ2iIwVjetWsXOP+uXLkC1jEoMiAgQKFQgPLi4uKgIQY7JjQ0dN++fffv33/8+PGcOXOgZ5mXl1dQUFD5hFASHsGshrMhDkhPKlY4COvW25EfUSIn435/hDgAzGFocBcvXgzDIaNHj3ZycoK+oFTKGIJgSp87dw7qSKgO58+fD8Z1//79wYnYokWL9957D5527twZfI1PnDAoKAhcieB0hG4l4oDstGK/IAUSEnY0MfaXr1IKc3UjZociu+fbD26/PbuOo6uAWmc7qhG7DvZX5QlujJV/Dq1LlykIQakQ2dUCew8/mVxJ7l6e2u//TK+w1Ol04HA2mQW2BXgBTVqa4eHha9euRdywXo/JLGdnZxgzNJkVExMDIzTIDIlXVU05G+qsNva1ZiX1dvGu5ffHfx1hrkDl7hoL3HK48SazoC9osIWfO/l6TGaBCx26mCaz4DcD1pLJrMObMhOv5L+7sA4SGHa3eGrLl8k6HRrySQiyS76bdPu1cSG1Inh0nj8bdrdmZeBHIQW5mrO/CXTpBqesm30vONJJgCpE9rmKb8yCOuf/yM7LtK+mYPPC+1IZ2efdACRI7HeB/fIpd7oMCIhsbhdLWDbMTfaqJRdysAe7DjmyYsqdwHDHPuMEWkk8L9bMSFQ6SwdPDUYCxt6DMK2fnVSk0r7Yw/uF9iIPAmaK3csfpCUWRjR27TqUK7v+eYHD0qFTe7PjTz9GBAqLdu422I8Qv2v17uXCs0eyc9JLnNxkwz6pLQpnMRZiKcd3PLx9WVVcoCNIpHCQuHrJnJxlEhmlMQ6JSejDdbKH+kib7CNJIooyKkASiKJJkqCosiCcUIDWB98k4V95ukRC6PSBOg2FmQNaH/9TQtD6LP1bECRJs28Bp6L1b0MbzSWSyUitFhXlaQvytUUFOngfNy9529e8gyMdkEjAQnyS0/uykm4UqgspjYa5Njqt6eujlxVBlP5XIcArYuMaszFh2QS9QGn9AcUIimAHaQgJovVLRwyF4QA0C6c0iFtfkNbfKcNTBuN3hCE7ULBCSbp5yeq+4BIl+GiIlcFC5Jvx48cPGjSoVatWCGMEDubON1qtlp0hhjEGXxG+wUI0Cb4ifIOFaBJ8RfhGo9HIZDKEqQgWIt/gGtEk+IrwDRaiSfAV4RssRJPgK8I3IETcR6wMFiLf4BrRJPiK8A0WoknwFeEbLEST4CvCN1iIJsFXhG/AoY2FWBl8RXiFpmmKoiQSTiIoixosRF7B7bI58EXhFSxEc+CLwit4xoM5sBB5BdeI5sAXhVewEM2BLwqvYCGaA18UXsFCNAe+KLyCjRVzYCHyCq4RzYEvCt+Yi+Vq52Ah8goM7qWnpyNMJbAQeQXa5Se2RsOwYCHyChaiObAQeQUL0RxYiLyChWgOLERewUI0BxYir2AhmgMLkVewEM2BhcgrWIjmwELkFRCiTqdDmErY485T1gUGV7AWK4OFyDe4dTYJFiLfYCGaBPcR+QYL0SRYiHyDhWgSLES+wUI0CRYi32AhmgTvPMUTsbGxJFlqGsI1h2N47Nmz55w5cxAGW8280ahRI8Rs6cgArkSCIAICAoYMGYIwerAQeeKtt95ycnIyTmncuHFkZCTC6MFC5InOnTsby87Ly2vgwIEIUwYWIn8MHz7c1dWVPa5Xr17Dhg0RpgwsRP54+eWXo6Ki4MDNzW3w4MEIY4QtW80Pk9TxcblFBTpKx2zBTeh/dHTpPvDMjvH63eARgQy7xzOlyredlyBKB68iaH2KREoYbyJOSglK/5Tdat6wF72JAmXbikPKo6ycK1evODs5gxHNviN8Hv17M5vesykUhQy7hpeeyrC/vQxRGkNihTIGSt9Ov1W5cckndzcvQyaXePjIW77qgayKzQpxw9zkwnytXEFq1VTpDWP3fmf3gdcrgHlk958v2z2eyTRsO8/uKk/SiGJeyerSgOEpTTD/nsitUJ4sPSebQtGUfv96ovQtaP3m9iRNUESFD2YksvL97aWI1j6ZqP8McCMNpdkN78uFWFrSKMUYmZKkdDT8ZsIbOncd6oushG0Kcc1n99y8Fd2GBSDMs5Gfqdu/NqVRG9dWPTyRNbBBIa6blewT4NjuTW+EsZBfFt+LauL6cj8raNHWjJWE88UlxTqswuoRGeuWcC4XWQObE+LFHAdH7AqoJi908FBrrNNC2to9K1JReB5+9ZEgnZYqUlnhCtra7ButDqAQpiZQBOIdPA0MUwlr2K+2JkQJiYxcaphqYY3+mq0JkWmWaSu0LDaD3sGOm2aMtSEYEeKmGWN19MPjiHdsTYhSCSGR4Ka5Bljp4tme+4Z+YhYMxnJw04yxNtaaemBrQmT62rhlrgEEYZUK0fZqRBrrsIbQiLSCI9HWxppp6zUutgJheuY3x+CJKtWh72udN276EQ527trauWtLxDvHjh/p0KnZ48c5VRczfE4LoK3TpmBjBVMRZuUE4h8sRIwgsMFJD6SFLYtOp9u+4+cNG1fDcXT9hsOHjWnYkFlil5h4Z9/+HRf/PZee/iC0dvirr/bt07s/qhbQRMJp799P3rlri7u7R6sXX37v/6bMXzDj9OkTwcG1hwwa2bVrD7YkpMAnSUpOdHNzj4iImjh+qp+fP5u16vtvfj9y0NHBsVOn7kFBtQ0n12q1a9auiDt7KjMzvUGD2H59/vfii21QtaGRxVfweWBrfUQdhSgLW5bVP3y7d+/2ObMXT//0cx8fv6mfjE9Ovgfpy1csOXfu74kTpi74Yhmo8JtlC+POnkbVQiaTbf1lQ0hI6OFfz4x6+/9+/W3fB5NGd+rY/cjhuA7tuyxaMjdflQ/Fzl84+9msD0GU27YemjljQUZG2tJlC9gz7N23Y+++7fBhVqzYGBAQuHHTD4aTL/v2yx07N/frO2Dzz/vbte00c/ZHJ07+iaoNYR1zz96Nlbz8vG3bf3rzzWHNm7340kvtpkye3qzpi9mPsiBrxowvFi1a0eSF5i/ENoO6MCqy/j/nzqDqUjeiXu9er8vl8vbtusDTmJhGIEGpVNqhfVeo0pKTEiFx7bqVbV/u2P/1QVAdQoFxYyfFxZ26kXANsnbt3tqubWfQmauLa/duveBTsactKSk5/PuBQQOHw8ndXN1efaUP6NtYpmLB1ppmcIERljQsrALq1Ythn4Iy5sxeVJpH07t2bT37z+mUlCQ2AaoiVF2gOmQP2FBMoaF12KcODo7wmJ+fB493794CqRleEhUZDY83blyF30Bqasor3XsbsiIj67MHN29eV6vVzZu1MmTFNm4KNW5uXi7oElUPbKzUHHCBWdSwFBQWwKNSoax0HurjTydqNOp3Rr0XG9vMxdll/MS3UQ0gKv4+yEpOY5VKBdWbwuiTODoyGi0sLACgI8tKlkWpdCh7FdOmV/5sOY+yqy9E7L55TlhwIR0dSm/2E+k3b92AqmjxohVNm7RgU+CW+3hzGAhBqWQkWFxcZEhhfyRent5QiUokkpKSYkNWUVEhe+Dl7QOPkydNCwwMNj6br68/qh54GthzgSRomrBgYCA8vC40x5f/u1i/fgOkj+X6ybT3O7Tr4u7BLDI3KO/evbvwF1bWnnIBfAxogq9e/c+Qwh6H16kLtamfXwDz9I3SLLCR2YOgwBCFQgEH0JFlU3JyHsG3YGvT6mClEVJbM1YomiAsmekOlU2Xzq+C1Qz9qn8vnf/2u0UXLpwFUYK/BpTxy7ZNYM2AEQ3pYM2kZ6QhLgHL99Tp4zt3boE3hQ+zYuVXYJTUjWACiIFlc/KvLmBiOwAAEABJREFUozCgAsdbtm64di2efQkIDhxDYJ3Ex1+CziLYy1M+Grf0mwWoJuDFUzXHUmMFAJ8I3LklX30O/bCIOpFzZi1iDYtpn84Dl16fvh2h1Zv2yVwwpWd8NmXYiP4b1u1A3ACOm4dZmb9s3/TdiiXgPgT7HXqobNaQwW/DgB78HubM/QTcnGBQfz5/Ohsu5s0Bb9WpE7l56/qLF/9xcnKOiW40efJ0VF1okCFhhVrR1mLf/LwouTBX9+aHYQhTLdbPuj1qVpiDmwTxiy3WiKSVujk2A4GNleeAFWYxQf/s02nvm8v9adMecFAj0UDT2H3zPKD5/0FDp2316s3mckWlQsTUhxQ2VmqMpQ7t50WAfy2EqQHYasYIAlwjYiqBjZWag2tEkWKDq/hohKvEmoFHVmoOrd/qAWHEhs0JkSZwH7Fm0Dg+IkYIEMgaoZ+xsYIRBNh9gxEEtiZEpYNEV4yVWH0kUgLJ+Z56g2xvYqynj1xTgjDVI/uBmiQJBwfEP7YmxA4DfErUWv2KIozFnDuc5eIhQ9bABtc1RzZ23bc8EWEs5Nal4uwHxYM/DkbWwDa3yb39X+Gfm9N9Q5yCIx0kElRFLGP9Xs2m7WyCLt3n4VlCVz6lDP3koqTK5Z/6Lk+eo+Lzii83yjM6NJQxfikpRQXZVNJ1VWG+evQX4chK2OzG4QkXCv85lFVYoNOodXRVQmT3BqdNLV8rTWT3fq9wpw0iLT+PvjRhzman2ZXNhlyCufAVz/C0dzF+CRsYt8Lm4uXx5Mo+dulReZw5w7Hxu0hkSCqVePsrX5tgze2tbVaIz87XX38Njx988AHihYkTJw4YMKB169aIA7Zt2wZfRyaTOTk5+fj4hIaGxsbG1teDhI1dCzE+Pr5hw4ZXr16NiYlBfDF37tzevXs3btwYcQOo/NatWyRJUvolE1APu7m5ubi47N27FwkYOw3CBD+/cePGpaenIyYeEn8qRExspxncqRDo0aMHGzSC1ANCzMvLS0lJQcLGHmvE7OxsuD23b99u0aIF4h1Qv4eHBxuegQuKioqGDh167949Q4qjo+PJkyeRsLGvGrGkpGTMmDFwqzw9Pa2iQmDq1KnwG0Cc4eDg0KVLF0PMJ2ig582bhwSPfQnx4MGDo0ePDgoKQtbDz8+v+oFpno3XXnvN358JwgQqvHjx4p49e1auXImEjV0IMTc3d8qUKUh/h5o2bYqsypdffhkWxm0gCrCX27dvDwe1ajFrC7/66iu5XD5+/HgkYOxCiHPmzHn77RpFN3yOpKamarVaxDGTJ0+GnuiBAwfYp/D1Bw0a1LFjx/v37yNBYsvGCpgFx48ff/PNN5GQAN/NqlWr2LqKZ8B8fuutt8aOHdutWzckMGy2RiwsLBw1alTbtm2RwIDem4NV5rcg5OrqCv1FsKBZH76gsMEaMS0tLT8/PzAwkI1WjanM5s2bjx49+uOPFm5KxSW2ViNev36dtYsFq8Lk5GTKGpvdGQP9RbBdWrVqdfPmTSQMbEeIDx48QHpP4f79+7n2j9SEIUOGFBcXI2sDozvQRs+aNQsaayQAbESIIL6ZM2fCAYzxI2EDZgo4U5AAkMlk0EZfuXLl888/R9ZG9H3Ex48fu7u779q1C3yECFMtdu/evWPHjo0bN0okVlitwiJuIf7www9w7UaOHInEQ1JSUu3atZHASEhIGDZs2Pfff8/phIwqEGvTDH3B7Oxs6PWLS4XQOxw8eDASHlFRUXFxccuWLduyZQuyBqIU4urVq8H2hBZ5zJgxSFRA+xMebrXp+E9lzZo1YPNNn179TQmqjfiEeOjQIXisW7euFTs01QZc2dAVQwIGxgbbtGkDHW7wxSIeEVMfEW4hjFDl5ua6uVV3lzlro9PpwN9u3ek/zwI0ONBlXLBgQcuWLREviKZGnDp1KjvxWLwqBB4+fPjuu+8iwRMSEnLs2DH45a9duxbxggiEePo0s133pEmT/ve//yGRQxCEAE1mcyxfvhyMQmisEfcIWoharbZ3797srHo/Pz8kfuBbwN1F4mHs2LFwC7p3756ZmYm4RLh9xPT0dBiBAH+HVWZMcYRarc7KyhLdN4LPDL3zhQsXNmzYEHGDQGtEGHqKj4/39PS0JRUi/comGIoU3SCCt7c3OCvAy5iRkYG4QaBChOoQrGNkc4CltWLFChgZt/oEnGpw6dIl7jpIONKDdUhJSSFJMjAwEImEW7duffbZZ9yNuwi0RtTpQbZLcHDwuHHjCgoKkEgAIcIgAuIMgQoR2q+ff/4Z2TR79+5NSEhQqVRIDNy5cyciIgJxhkCFyF0gBEHRpEmT1NTUM2fOIMEDNSKnQhRoDO3Ro0cj+yAqKmrChAmNGjVydnZGAub27dv2WCPafB/RGHCL5OXlCXbFMdJHKIAhFl9fX8QZAhUijHKuWrUK2Q3gLs3JybHWXMCnwnV1iITcRyTsbOMeGLR48OABeLyR8OBBiNiPKCwKCwtv3LgBRgwSEvPmzWvQoEHfvn0RZ+A+orBwdHRUKpXz589HQgJqRE6diEiwQty9e/eiRYuQXRIdHV2vXj0kJOy3jyiXy+2tj2gMuzR23759SADAaKSPjw/Xnl2BCrF3795Tp05F9g2YL2xYR+vC9eAei0CFSFEUD0EEBU5YWNjw4cORteGhXUaCFeKRI0fYECJ2DtiqqGwnGGth10KUyWQkaadbb1QG6kUrLrnip2nGfkRxkJ+f7+LiAt0VqZSZHtC9e3f4re7fvx9xDIzsdezYkV2/xim4jygOQIVIv/q9oKCgZ8+eWVlZMCR4+PBhxDE8eBBZBCrEuLg4flYxiotvvvnmlVdeYTfMgsHAP//8E3EM17O/DAi3j2jPfkRzDBgwAMYA2WO4PgkJCawouYMfSwUJVojNmzdfunQpwhgxaNCgO3fuGKdkZGScOHECcQk/lgoSrBDBhNJoNAhjBPSbg4KCjENPqdVq8HMhLuF6hYABgc7Qjo+PhxqRt8AromDr1q0XL148d+7c2bNnVSpVWlqan1MTOs/zyK6bAbX8kX6TcYLWb0COyjYG16fQCJVvKc5uJk7oi7PbkENdRJXuJV6+5TjBJKry8kO926VcI1JQXmmxipuOV9jmXA9JIMooiSQJ3yCFd+DTQzULy30zatQouMTwkeARrEJfX1+oBqBX9McffyCMEetm3y3M0xEk0mkZKZCM3vQS1EuNYORQpqoy5ZVJBnRCEmXa08uuXAP6Q7ZvXr7vfVkWe3K9hvVvp3/C6LmCNMuKsUhlcEZCJicaveTR8lV3ZB5h1YjR0dE//fSTwZXNzp6HEXeEMWL1x3e9Qxz6jwtAgogJ/3SunsmNP/MoIFQREm12pyNh9RGHDBlSOXagtfazFSarP71bv7lXl8GiUSEQ09ptwJSwQxvSzv9uNnqHsIQIbXGPHj2MU7y8vIQZdNoq/LohUyqTxHYWZYTI+i3dL53INpcrOKt54MCBxpVibGxsZGQkwujJSC72DlAicdKkk6dGQ6vNxBMQnBBdXV179erFjqh6enoOHToUYcrQlGilShHPBaEolJVhenWYEL+VoVJsoAdhytCqaa1axO5VSkdTZmYQ1MhqLilCfx/MyrhXXJCvURczXgN4J9aAB88CRZd5qhDztNSHxTi6GO8WTZWlGx0Qeo8CHLSv/YUmUC2XKFZ+dFefbXCMse4B/XOi1DXBZhm8DvqzEbSRO0siBT+CBGxxRxdJUKRD655eCCMwqinE3zZkJN8o0JTQpJSUwC1WSBUuBOMiZbyqiK7gdip9avBZsi5TmlUkquDHNHbGEoQjTdPGkqtQjC57LPfdlp4WVfZmSSXwo9CWaB9lajLvF188miNXktEt3dr0wYrklVJvuSksFuKv6zLuXlVJJISLj3NgjChvJKWmk69k/nfqMfy90NG91Sui+RZinwdSRXBSy4T4/dREqPFqNwxw9hVxtC5SToQ2YSKfPryT++/RnGtn8t+eG4rEgNgnMZPVyqpA8o2ibz+47eLrVK9diKhVaIxPHbfojqGEVLJi8h2E4QVzv6VnEmLuQ+2+1anRncJqRdtgpyq8RS3/KJ8VU0SgRbqKTpZIMDfN9OlCvHO58Ocvkxp0CRPh1nfPimewU1jz4OVTbiNhQxLilqHeEjVdJT5diL9tSItoEYxsHQdXiU9tz9Wf3ENChmbnaokZM7+kpwjxx+n3XP2d5c62Wxka4RvhBt6oX5YIN2Cm2I2VKqrzqoR4Yke2Wk0FN/RGdkNE60BwNKYlqhGGG2hkeR/xyt85vmEeyM5w8XI8tC4VYThAP6XWwj7i6T3MjB3vUFckSC7F/zFlRktVQQ563oQ29SvM1+ZmCzE6IwyE8r+4se9rnTdu+hE9D6ow+s0K8dblfCdPR2SXyBTS3zelIeEBo/CWLu2YPefjQ7/uRcKgio9uVoiqXK1fuCeyS2D0MutBCbIJEhKuIcFAGB4qYXqI7/pZFZR3cJchbriX/N/vx35MuX/N2cmjflSbrh1GKZVOkH46bvuRE2vHjly5cesnGZl3A/wi2rYe2LxJT/ZVB3779vzlQwq54wuNuvl6hyDO8I/wyEm1hS0pO3RqBo+LFs9duerr/XuPI2YX9hMbNq5OSk50c3OPiIiaOH6qn58/W7iKLBaojHfu2nL48IGU+0m1Q8KaNXtx5IixEkvcyxb7Ee9dU0mkXLlssrJTvl8/XqMpeW/0j8MGLUzLuLVy7VidjpmnJpHKiory9xxc/L++ny6aE9eoQcdte+blPGaCGZz5Z+eZf3a81uPDiWPWeXnUOnJsDeIMCROulkg4J47Nyargt0NM8KQPp8xgVXj+wtnPZn3YtWuPbVsPzZyxICMjbemyBWzJKrIM7Nq19aef1/Z/fdDWzQd69Xr94KE9W3/ZiCyB6d+SlljN+TlaiYyrTvHFy79JJbLhAxf6+YT6+4a/0WdaalrClesn2FydTtOlw6jawQ1BCs1ie8CvMDXtJqSf+ntbo5hOIE1HR1eoIyPCmyEuAYdiZmoxEhj6+ZrVvy9r161s+3JHUBLUeTExjcaNnRQXd+qGvu2uIsvA5f8uRkVFd+vW093do2ePfsu/W9+yxUvIEpgOrpkZOKaFqNVSBGeTt6FdDg6KdnIqXeXq6RHg5RmUmHTJUCAkMIY9cHRgbPai4nz4AlmPUvx8wwxlgmpxG+4cvAyFBYILR0bRZt0fz8Ldu7fq1YsxPI2KjIbHGzeuVp1loEGDxhcunP1y0ZzfDu/PzcsNrBUUEWHpciLCnMViuo9IEDR3+1oXFatSUq+B88U4MS8/2+jdn/zRF5cUUJROoSi34uVyB8QpJCEhBTeeRKDqD/CpVKqSkhKFonztlaMjcz0LCwuqyDI+A9SXjo5Op8+cWPjlbKlU2r59lzHvTPD2tmDVucUTY2VyKYm4cqS5uHiF1Y7t1rHCto9OTlUtkVQqnEhSotGUt5Ul6kLEJTRFKw7TK0UAAAWWSURBVB1sKmStUsnorLi4fO1SgV5nXp7eVWQZn4EkSWiR4e/evbsXL/6zfuPqggLV/HkWhFUuj4VSCdNCdPeRZ6dx1VWv5Vf3wuVD4aEvGCI6pGfe9fGqygqGOtLDPeBecny7sj7J9QRuY5hSFO0fynGlyy9Qh0VF1r969T9DCnscXqduFVnGZwB7OTKyflhYndDQcPjLV+UfPLQbWQJBWOjQDm/gpNVwVSOCR4aiqH2/fq1WF2c+TDpw+Lsl3w1Ky3jKFKzGDTrHXzsGAypwfPSvjUn3ryDO0BTooE8dESs4fz6hX3j27OUVCoWPj+/583H/Xjqv1Wr79R1w6vTxnTu35OXnQcqKlV81eaF53YgoKFlFloE/j/4GlvWZMyehgwimzF+njjaIaYwsgaLNfnrTNWJ4I7gHRH5WiYv385+MDWbvlPc2H/tr09JVwzIf3gsJinmj77SnGh+d240oKMjZc2jJT9umQcve+5X3N2//jKMIUhl3mdVVSHjQzBe27CsPHjRy3fpV/5w7s2XzAfDOPMzK/GX7pu9WLAEfYbOmL74z6j22WBVZBiZPmv7d8sXTZkxCzJJzL2ij3+g/BFkCYX4CkdloYBvmJukoSXiLAGR/JJxI8a+t7DPWHwmMlR/dCYxw6DCgFhInG2bd7js2MCjSRJ/H7O++cVuPYpWNDHNZikat7TNGcCq0ASw2VoDYdq5/H3iYdiMnoJ7pmWCPczMWfzfIZJaDwrmoxLSt4+8T/t7oH9DzY/rnncxlwWiNRGLiC4aGNBo11Kytd+efdFdPuVBD6YqbKtxPVS0nbfmqd9yv2eaE6OLsNWncJpNZYIXI5aZjBZHkc47IaO4zMB9DUyKXmejjSiVVRXQryi0a/gUfwXqrAUEiUiLmVSuE2VlsVcmiSQe3yyceJ55PC2tmoqcIlY2nh/U7K8/3M9w8mRJc11Ei1OWyjKmiE/FygSpsrae0QCNm1S7OV+emces9Fgj3rzwkpXSfsQI2BUS/dMosT+8Kjf0iPOVqJrJ10q7n5D8sGDU3DGGswTP0ySVo7MI68UcSHz2w2Xox5XJWflb+2C/rIOEj5vg3jLFiRnHPZByC6Tn+q4j06xl3zwlxAn0NSfgrpeBxwej5IqkLxbyklFk8ZdE0MJOMWxyBKO31Y0npN5//kiWrcO9S5pU/Et3cpe8uCEcYq2KZM2XkrNCzvz++fPxRzoM8pbPSt46Hk4d4gtuXkZOqyk7KKy5UKxzIfmOCAyPFFVNK3E3zc4uP2LKrO/xd+DM3/vTjpIsPmDNLCAkTCZOGA+NpjE/E6iyDZicclkfaNAqqSZTtf4RKQ36Wvrx0W6VKcTiNooBWaLKMYoEyjyR8Oh1BUbSOnclB027eis4DAkMbiDEwurjt5uo4tKugaSc3+IOD2/8W3o7PU+XoVLkaxk1kLMRKYYn1ymB8SSTJxPVm09ljFlIf4ZgpzEjnyfMYSpbHOSYZhZFlcZENMmW3WqLKTi6VEfAjUSglHn6OMS3dAurYSFg90WG0Z9WT1HScI+IFR/hDGEzNEOimkBiTyOQSqUzEAbGkUgKZWYCBhSgmZEqipJC71UScA12zoHDT1i2eZCImQuu7ZKeLdW7emX1ZCgcJMlOhYyGKiXave8INO7pZlCOuSVfzOr7hay5XWPs1Y56FjfOSwK3QpIN37RgRuJ9Uj+mLfzxMupE/bHqok5vZDi4WoijZvjT1Ubpap6V0Vc8Ko5/N//2MxdiyzNbiz1qalDCbmDs4S7sO9qsVUdXPBgtRzKhRUZHRYssKYwNl/xmPARBlu8aVFaMJQ4QuusI29PpM1u1blkJXGDwoT9GfktTv5kMYv6M+XSJxcEbPAhYiRhBg9w1GEGAhYgQBFiJGEGAhYgQBFiJGEGAhYgTB/wMAAP//Uk9SQwAAAAZJREFUAwBg+Ey0plPJmQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x11d4a5940>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44011653",
   "metadata": {},
   "source": [
    "## Ejecutar consultas al Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ToolException",
     "evalue": "Unknown tool: install_policy",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mToolException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m email_id = \u001b[33m\"\u001b[39m\u001b[33mMSG-001\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m prompt = (\n\u001b[32m      4\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33memail_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memail_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSoy ana@empresa.com. Necesito instalar Docker en mi portátil corporativo para un proyecto.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m¿Puedes abrir la solicitud de IT por mí?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m out = \u001b[38;5;28;01mawait\u001b[39;00m graph.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=prompt)]})\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(out[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3161\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3158\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3159\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3161\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3162\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3163\u001b[39m     config,\n\u001b[32m   3164\u001b[39m     context=context,\n\u001b[32m   3165\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3168\u001b[39m     print_mode=print_mode,\n\u001b[32m   3169\u001b[39m     output_keys=output_keys,\n\u001b[32m   3170\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3171\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3172\u001b[39m     durability=durability,\n\u001b[32m   3173\u001b[39m     **kwargs,\n\u001b[32m   3174\u001b[39m ):\n\u001b[32m   3175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3176\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2974\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2972\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2973\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2974\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2975\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2976\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2977\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2978\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2979\u001b[39m ):\n\u001b[32m   2980\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2982\u001b[39m         stream_mode,\n\u001b[32m   2983\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2986\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2987\u001b[39m     ):\n\u001b[32m   2988\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:138\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    140\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:845\u001b[39m, in \u001b[36mToolNode._afunc\u001b[39m\u001b[34m(self, input, config, runtime)\u001b[39m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m call, tool_runtime \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tool_calls, tool_runtimes, strict=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    844\u001b[39m     coros.append(\u001b[38;5;28mself\u001b[39m._arun_one(call, input_type, tool_runtime))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m845\u001b[39m outputs = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*coros)\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._combine_tool_outputs(outputs, input_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:1176\u001b[39m, in \u001b[36mToolNode._arun_one\u001b[39m\u001b[34m(self, call, input_type, tool_runtime)\u001b[39m\n\u001b[32m   1172\u001b[39m config = tool_runtime.config\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._awrap_tool_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap_tool_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1175\u001b[39m     \u001b[38;5;66;03m# No wrapper - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_tool_async(tool_request, input_type, config)\n\u001b[32m   1178\u001b[39m \u001b[38;5;66;03m# Define async execute callable that can be called multiple times\u001b[39;00m\n\u001b[32m   1179\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(req: ToolCallRequest) -> ToolMessage | Command:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:1125\u001b[39m, in \u001b[36mToolNode._execute_tool_async\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m   1122\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1124\u001b[39m     \u001b[38;5;66;03m# Error is handled - create error ToolMessage\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m     content = \u001b[43m_handle_tool_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_tool_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ToolMessage(\n\u001b[32m   1127\u001b[39m         content=content,\n\u001b[32m   1128\u001b[39m         name=call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1129\u001b[39m         tool_call_id=call[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1130\u001b[39m         status=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1131\u001b[39m     )\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# Process successful response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:429\u001b[39m, in \u001b[36m_handle_tool_error\u001b[39m\u001b[34m(e, flag)\u001b[39m\n\u001b[32m    427\u001b[39m     content = flag\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(flag):\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     content = \u001b[43mflag\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [assignment, call-arg]\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    431\u001b[39m     msg = (\n\u001b[32m    432\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot unexpected type of `handle_tool_error`. Expected bool, str \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    433\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor callable. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    434\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:386\u001b[39m, in \u001b[36m_default_handle_tool_errors\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ToolInvocationError):\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m e.message\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:1082\u001b[39m, in \u001b[36mToolNode._execute_tool_async\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1081\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m tool.ainvoke(call_args, config)\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m   1084\u001b[39m         \u001b[38;5;66;03m# Filter out errors for injected arguments\u001b[39;00m\n\u001b[32m   1085\u001b[39m         injected = \u001b[38;5;28mself\u001b[39m._injected_args.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_core/tools/structured.py:67\u001b[39m, in \u001b[36mStructuredTool.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coroutine:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# If the tool does not implement async, fall back to default implementation\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(config, \u001b[38;5;28mself\u001b[39m.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:642\u001b[39m, in \u001b[36mBaseTool.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    636\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     **kwargs: Any,\n\u001b[32m    640\u001b[39m ) -> Any:\n\u001b[32m    641\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arun(tool_input, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:1117\u001b[39m, in \u001b[36mBaseTool.arun\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_tool_error(error_to_raise)\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m   1119\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:1083\u001b[39m, in \u001b[36mBaseTool.arun\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m   1080\u001b[39m         tool_kwargs[config_param] = config\n\u001b[32m   1082\u001b[39m     coro = \u001b[38;5;28mself\u001b[39m._arun(*tool_args, **tool_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(coro, context)\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1085\u001b[39m     msg = (\n\u001b[32m   1086\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSince response_format=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1087\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33ma two-tuple of the message content and raw tool output is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1088\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected. Instead, generated response is of type: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1089\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_core/tools/structured.py:121\u001b[39m, in \u001b[36mStructuredTool._arun\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.coroutine):\n\u001b[32m    120\u001b[39m         kwargs[config_param] = config\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coroutine(*args, **kwargs)\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# If self.coroutine is None, then this will delegate to the default\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# implementation which is expected to delegate to _run on a separate thread.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._arun(\n\u001b[32m    126\u001b[39m     *args, config=config, run_manager=run_manager, **kwargs\n\u001b[32m    127\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/tools.py:414\u001b[39m, in \u001b[36mconvert_mcp_tool_to_langchain_tool.<locals>.call_tool\u001b[39m\u001b[34m(runtime, **arguments)\u001b[39m\n\u001b[32m    405\u001b[39m request = MCPToolCallRequest(\n\u001b[32m    406\u001b[39m     name=tool.name,\n\u001b[32m    407\u001b[39m     args=arguments,\n\u001b[32m   (...)\u001b[39m\u001b[32m    410\u001b[39m     runtime=runtime,\n\u001b[32m    411\u001b[39m )\n\u001b[32m    412\u001b[39m call_tool_result = \u001b[38;5;28;01mawait\u001b[39;00m handler(request)\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_call_tool_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_tool_result\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/LIL Repos/aplicaciones_modelos_lenguaje_IA_generativa_4222155/agent_notebooks/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/tools.py:189\u001b[39m, in \u001b[36m_convert_call_tool_result\u001b[39m\u001b[34m(call_tool_result)\u001b[39m\n\u001b[32m    187\u001b[39m             error_parts.append(item.get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    188\u001b[39m     error_msg = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_parts) \u001b[38;5;28;01mif\u001b[39;00m error_parts \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(tool_content)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ToolException(error_msg)\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Extract structured content and wrap in MCPToolArtifact\u001b[39;00m\n\u001b[32m    192\u001b[39m artifact: MCPToolArtifact | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mToolException\u001b[39m: Unknown tool: install_policy",
      "During task with name 'tools' and id '9d23d7cf-153c-6613-65df-12e7245d5076'"
     ]
    }
   ],
   "source": [
    "email_id = \"MSG-001\"\n",
    "\n",
    "prompt = (\n",
    "    f\"email_id={email_id}\\n\"\n",
    "    \"Soy ana@empresa.com. Necesito instalar Docker en mi portátil corporativo para un proyecto.\\n\"\n",
    "    \"¿Puedes abrir la solicitud de IT por mí?\"\n",
    ")\n",
    "\n",
    "out = await graph.ainvoke({\"messages\": [HumanMessage(content=prompt)]})\n",
    "print(out[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
